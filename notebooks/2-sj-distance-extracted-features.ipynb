{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how tiles that ought to be similar differ. Use a pretrained model (e.g. SwAV/SimCLR), calculate the nearest neighboars of the embeddings and show corresponding tiles of nearest embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration is drawn from https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scistor/guest/sjg203/.conda/envs/pmchhg/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from lightly.data import LightlyDataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dpat.data import PMCHHGImageDataset\n",
    "from dpat.extract_features.models import SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwAV.load_from_checkpoint(\n",
    "    \"/scistor/guest/sjg203/projects/pmc-hhg/dpat/checkpoints/swav-epoch=199-step=39200.ckpt\"\n",
    ")\n",
    "# model = SwAV()\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"val\"\n",
    "root_dir = \"/scistor/guest/sjg203/projects/pmc-hhg/images-tif\"\n",
    "dataset = LightlyDataset.from_torch_dataset(\n",
    "    PMCHHGImageDataset(\n",
    "        root_dir=root_dir,\n",
    "        image_paths_and_targets=\"/scistor/guest/sjg203/projects/pmc-hhg/images-tif/\"\n",
    "        \"splits/\"\n",
    "        \"medulloblastoma+pilocytic-astrocytoma_\"\n",
    "        f\"pmc-hhg_{fold}-subfold-0-fold-0.csv\",\n",
    "        mpp=0.2,\n",
    "        tile_size_x=224,\n",
    "        tile_size_y=224,\n",
    "        tile_overlap_x=0,\n",
    "        tile_overlap_y=0,\n",
    "        tile_mode=\"overflow\",\n",
    "        crop=False,\n",
    "        mask_factory=\"no_mask\",\n",
    "        mask_foreground_threshold=None,\n",
    "        mask_root_dir=None,\n",
    "        transform=Compose([ToTensor()]),\n",
    "    )\n",
    ")\n",
    "val_dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [00:38<00:00, 16.98it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(model, dataloader):\n",
    "    \"\"\"Generates representations for all images in the dataloader with\n",
    "    the given model\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img, _, _ in tqdm(dataloader, total=len(val_dataloader), desc=\"Extracting features\"):\n",
    "            img = img.to(model.device)\n",
    "            emb = model.backbone(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "    embeddings = F.normalize(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "model.eval()\n",
    "embeddings = generate_embeddings(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_examples(embeddings, dataset, samples_idx = None, n_neighbors=10, num_examples=10):\n",
    "    \"\"\"Plots multiple rows of random images with their nearest neighbors.\"\"\"\n",
    "    # lets look at the nearest neighbors for some samples\n",
    "    # we use the sklearn library\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    # Get random samples\n",
    "    if samples_idx is None:\n",
    "        samples_idx = torch.randint(len(indices), size=(num_examples,))\n",
    "\n",
    "    # loop through our randomly picked samples\n",
    "    for idx in samples_idx:\n",
    "        fig = plt.figure(figsize=(n_neighbors, 1))\n",
    "\n",
    "        # loop through their nearest neighbors\n",
    "        for plot_x_offset, neighbor_idx in enumerate(indices[idx]):\n",
    "            ax = fig.add_subplot(1, len(indices[idx]), plot_x_offset + 1)\n",
    "            neighbour_sample = dataset[neighbor_idx]\n",
    "            metadata = dataset.dataset.get_metadata(neighbor_idx)\n",
    "            ax.imshow(torch.permute(neighbour_sample[0], (1, 2, 0)))\n",
    "            ax.set_title(f'd={distances[idx][plot_x_offset]:.3f}')\n",
    "            ax.text(0.5,-0.3, metadata[\"case_id\"].split(\"_\")[-1], size=12, ha=\"center\", transform=ax.transAxes)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    return samples_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_idx = plot_knn_examples(embeddings.cpu(), dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwAV()\n",
    "model.eval()\n",
    "embeddings = generate_embeddings(model, val_dataloader)\n",
    "plot_knn_examples(embeddings.cpu(), dataset, samples_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmchhg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
